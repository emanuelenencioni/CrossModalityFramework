#!/usr/bin/env python3
# -*- coding:utf-8 -*-

"""
Test script for CityscapesEvaluator to verify mAP calculation correctness.
This script uses the real Cityscapes dataset with actual images and ground truth 
to test the evaluator with perfect and imperfect predictions.
"""

import os
import sys
import numpy as np
import torch
from torch.utils.data import DataLoader, Subset
import tempfile
import json
from pathlib import Path
import random

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from evaluator.cityscapes_evaluator import CityscapesEvaluator
from evaluator.dsec_det_classes import DSEC_DET_CLASSES
from dataset.cityscapes import CityscapesDataset


def initialize_real_cityscapes_dataset(split_file=None, data_root=None, num_samples=None):
    """Initialize real CityscapesDataset from split file or directory."""
    
    # Default paths - modify these according to your setup
    if split_file is None:
        split_file = "./dataset/cs_train.txt"
    
    # Try to use split file first
    if os.path.exists(split_file):
        print(f"Using split file: {split_file}")
        
        # Read split file to understand structure
        with open(split_file, 'r') as f:
            sample_lines = [f.readline().strip() for _ in range(3)]
        
        if sample_lines:
            sample_path = sample_lines[0]
            if '/cityscapes/leftImg8bit/' in sample_path:
                root_idx = sample_path.find('/cityscapes/leftImg8bit/')
                data_root = sample_path[:root_idx]
                img_dir = sample_path[:root_idx] + '/cityscapes/leftImg8bit/train/'
                ann_dir = sample_path[:root_idx] + '/cityscapes/gtFine/train/'
                
                print(f"  Data root: {data_root}")
                print(f"  Image dir: {img_dir}")
                print(f"  Annotation dir: {ann_dir}")
                
                try:
                    dataset = CityscapesDataset(
                        pipeline=[],
                        img_dir=img_dir,
                        ann_dir=ann_dir,
                        split=split_file,
                        data_root=None,
                        test_mode=True,
                        load_bboxes=True,
                        extract_bboxes_from_masks=True,
                        bbox_min_area=500,  # Minimum bbox area
                        custom_classes=True,
                    )
                    
                    print(f"‚úì Loaded dataset with {len(dataset)} samples")
                    
                    # Limit number of samples if specified
                    if num_samples and num_samples < len(dataset):
                        indices = random.sample(range(len(dataset)), num_samples)
                        dataset = Subset(dataset, indices)
                        print(f"‚úì Limited to {num_samples} random samples")
                    
                    return dataset
                    
                except Exception as e:
                    print(f"‚ùå Error loading from split file: {e}")
    
    # Fallback: try direct directory loading
    if data_root is None:
        data_root = "./data"
    
    img_dir = os.path.join(data_root, "cityscapes/leftImg8bit/train")
    ann_dir = os.path.join(data_root, "cityscapes/gtFine/train")
    
    if not os.path.exists(img_dir):
        print(f"‚ùå Image directory not found: {img_dir}")
        return None
    if not os.path.exists(ann_dir):
        print(f"‚ùå Annotation directory not found: {ann_dir}")
        return None
    
    try:
        print(f"Loading dataset from directories:")
        print(f"  Images: {img_dir}")
        print(f"  Annotations: {ann_dir}")
        
        # Find a subdirectory (Cityscapes has city subdirs)
        city_dirs = [d for d in os.listdir(img_dir) if os.path.isdir(os.path.join(img_dir, d))]
        if city_dirs:
            # Use first city for testing
            city = city_dirs[0]
            city_img_dir = os.path.join(img_dir, city)
            city_ann_dir = os.path.join(ann_dir, city)
            
            print(f"  Using city: {city}")
            print(f"  City images: {city_img_dir}")
            print(f"  City annotations: {city_ann_dir}")
            
            dataset = CityscapesDataset(
                pipeline=[],
                img_dir=city_img_dir,
                ann_dir=city_ann_dir,
                data_root=None,
                test_mode=True,
                load_bboxes=True,
                extract_bboxes_from_masks=True,
                bbox_min_area=500,
                custom_classes=True,
            )
            
            print(f"‚úì Loaded dataset with {len(dataset)} samples")
            
            # Limit number of samples if specified
            if num_samples and num_samples < len(dataset):
                indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))
                dataset = Subset(dataset, indices)
                print(f"‚úì Limited to {len(indices)} samples")
            
            return dataset
        else:
            print(f"‚ùå No city directories found in {img_dir}")
            return None
            
    except Exception as e:
        print(f"‚ùå Error loading dataset: {e}")
        import traceback
        traceback.print_exc()
        return None


class PerfectModel:
    """Mock model that returns perfect predictions (identical to ground truth)"""
    
    def __init__(self):
        pass
    
    def eval(self):
        """Set model to eval mode"""
        pass
    
    def __call__(self, input_tensor):
        """Forward pass - returns perfect predictions"""
        batch_size = input_tensor.shape[0] if hasattr(input_tensor, 'shape') else 1
        return torch.zeros((batch_size, 1, 5)), None


def extract_ground_truth_from_sample(sample):
    """
    Extract ground truth bboxes from real dataset sample using the dataset's methods.
    This properly handles the padding and scaling transformations.
    Returns format expected by CityscapesEvaluator: [class_id, x, y, w, h]
    """
    # The dataset should provide properly transformed bboxes in the 'BB' key
    bboxes = sample.get('BB', torch.empty((0, 5)))
    
    if not torch.is_tensor(bboxes):
        bboxes = torch.tensor(bboxes, dtype=torch.float32)
    
    # Filter out invalid bboxes (class_id < 0 indicates empty slots)
    if len(bboxes.shape) == 2 and bboxes.shape[1] >= 5:
        valid_mask = bboxes[:, 0] >= 0  # Valid class_id
        valid_bboxes = bboxes[valid_mask]
        
        # Keep format as [class_id, x, y, w, h] - this is what CityscapesEvaluator expects
        if len(valid_bboxes) > 0:
            return valid_bboxes
        else:
            return torch.empty((0, 5))
    else:
        return torch.empty((0, 5))

def create_perfect_predictions(ground_truth_batch):
    """Create perfect predictions that exactly match ground truth."""
    perfect_predictions = []
    
    for gt_bboxes in ground_truth_batch:
        if len(gt_bboxes) == 0:
            perfect_predictions.append(None)
            continue
            
        # Ground truth format: [class_id, x_center, y_center, w, h]
        # Prediction format: [x_center, y_center, w, h, obj_conf, class_conf, class_pred]
        pred_bboxes = torch.zeros((len(gt_bboxes), 7))
        
        for i, bbox in enumerate(gt_bboxes):
            class_id, x_center, y_center, w, h = bbox[0], bbox[1], bbox[2], bbox[3], bbox[4]
            
            pred_bboxes[i, 0] = x_center  # x1
            pred_bboxes[i, 1] = y_center  # y1
            pred_bboxes[i, 2] = w  # x2
            pred_bboxes[i, 3] = h  # y2
            pred_bboxes[i, 4] = 1.0  # Perfect objectness confidence
            pred_bboxes[i, 5] = 1.0  # Perfect class confidence  
            pred_bboxes[i, 6] = class_id  # Copy class_id
        
        perfect_predictions.append(pred_bboxes)
    
    return perfect_predictions


def create_imperfect_predictions(ground_truth_batch, noise_level=2, conf_range=(0.7, 1.0)):
    """Create imperfect predictions with spatial noise and confidence variation."""
    imperfect_predictions = []
    
    for gt_bboxes in ground_truth_batch:
        if len(gt_bboxes) == 0:
            imperfect_predictions.append(None)
            continue
            
        pred_bboxes = torch.zeros((len(gt_bboxes), 7))
        
        for i, bbox in enumerate(gt_bboxes):
            class_id, x, y, w, h = bbox[0], bbox[1], bbox[2], bbox[3], bbox[4]
            
            # Add spatial noise
            noise = torch.randn(4) * noise_level  # Only need 4 noise values for x,y,w,h
            x_noisy = x + noise[0]
            y_noisy = y + noise[1]
            w_noisy = w + noise[2]
            h_noisy = h + noise[3]
            
            # Ensure valid dimensions (positive width and height)
            w_noisy = max(5.0, w_noisy)  # Minimum width of 5 pixels
            h_noisy = max(5.0, h_noisy)  # Minimum height of 5 pixels
            
            # Ensure center coordinates are within image bounds (assuming 512x512 processed size)
            # Keep some margin from edges to ensure bbox doesn't go outside
            margin_x = w_noisy / 2
            margin_y = h_noisy / 2
            x_noisy = max(margin_x, min(x_noisy, 512 - margin_x))
            y_noisy = max(margin_y, min(y_noisy, 512 - margin_y))
            
            # Variable confidence for both objectness and class
            obj_conf = conf_range[0] + torch.rand(1).item() * (conf_range[1] - conf_range[0])
            class_conf = conf_range[0] + torch.rand(1).item() * (conf_range[1] - conf_range[0])
            
            # Prediction format: [x_center, y_center, w, h, obj_conf, class_conf, class_pred]
            pred_bboxes[i, 0] = x_noisy      # x_center
            pred_bboxes[i, 1] = y_noisy      # y_center
            pred_bboxes[i, 2] = w_noisy      # width
            pred_bboxes[i, 3] = h_noisy      # height
            pred_bboxes[i, 4] = obj_conf     # Objectness confidence
            pred_bboxes[i, 5] = class_conf   # Class confidence
            pred_bboxes[i, 6] = class_id     # Class prediction
        
        imperfect_predictions.append(pred_bboxes)
    
    return imperfect_predictions

def create_img_meta_from_sample(sample, img_id):
    """Create proper img_meta dictionary from sample using dataset information."""
    img_metas = sample.get('img_metas', None)
    
    if img_metas is not None and hasattr(img_metas, 'data'):
        # Use existing img_metas if available
        return img_metas
    
    # Fallback: create img_meta from available information
    padding_info = sample.get('padding_info', {})
    
    # Get original shape from padding info or use defaults
    if padding_info and 'original_size' in padding_info:
        orig_width, orig_height = padding_info['original_size']
    else:
        orig_width, orig_height = 2048, 1024  # Cityscapes default
    
    # Create img_meta similar to what the dataset creates
    img_meta_dict = {
        'orig_shape': (orig_height, orig_width),  # Original image size (H, W)
        'img_shape': (512, 512),  # Current processed size
        'pad_shape': (512, 512),
        'ori_shape': (512, 512),
        'img_id': img_id,
        'idx': img_id,
        'filename': f'test_sample_{img_id}.png'
    }
    
    # Create DataContainer-like object
    class SimpleDataContainer:
        def __init__(self, data):
            self.data = data
    
    return SimpleDataContainer(img_meta_dict)


def test_real_dataset_perfect_predictions():
    """Test evaluator with perfect predictions using real Cityscapes data."""
    
    print("=" * 80)
    print("CITYSCAPES EVALUATOR TEST - REAL DATASET WITH PERFECT PREDICTIONS")
    print("=" * 80)
    
    # Load real dataset
    print("Loading real Cityscapes dataset...")
    dataset = initialize_real_cityscapes_dataset(num_samples=20)
    
    if dataset is None:
        print("‚ùå Failed to load dataset. Please check your data paths.")
        return False
    
    # Create dataloader - no collate_fn to get individual samples
    dataloader = DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=lambda x: x)
    
    # Create evaluator
    print("Initializing CityscapesEvaluator...")
    evaluator = CityscapesEvaluator(
        dataloader=dataloader,
        img_size=[512, 512],
        confthre=0.001,  # Very low threshold for perfect predictions
        nmsthre=0.65,
        num_classes=8,
        per_class_AP=True,
        per_class_AR=True,
        device="cpu"
    )
    
    print(f"Testing with perfect predictions on real data...")
    print(f"Number of detection classes: 8")
    print(f"Class names: {DSEC_DET_CLASSES}")
    print()
    
    # Test each batch
    ap_50_95_scores = []
    ap_50_scores = []
    total_gt_objects = 0
    
    for batch_idx, batch in enumerate(dataloader):
        print(f"Processing batch {batch_idx + 1}/{len(dataloader)}...")
        
        # Extract ground truth from real samples
        batch_gt = []
        batch_img_info = []
        
        # Get dataset reference for original coordinates
        dataset_ref = dataset.dataset if hasattr(dataset, 'dataset') else dataset
        
        for sample_idx, sample in enumerate(batch):
            # Use the dataset's properly transformed bboxes
            gt_bboxes = extract_ground_truth_from_sample(sample)
            batch_gt.append(gt_bboxes)
            total_gt_objects += len(gt_bboxes)
            
            # Create proper image metadata
            img_meta = create_img_meta_from_sample(sample, batch_idx * len(batch) + sample_idx)
            batch_img_info.append(img_meta)
        
        # Skip batches with no ground truth objects
        batch_gt_count = sum(len(gt) for gt in batch_gt)
        if batch_gt_count == 0:
            print(f"  Skipping batch {batch_idx + 1} - no ground truth objects")
            continue
        
        # Generate perfect predictions
        perfect_preds = create_perfect_predictions(batch_gt)
        
        # Calculate metrics
        try:
            ap_50_95, ap_50, summary = evaluator.calculate_coco_metrics(
                perfect_preds, batch_gt, batch_img_info
            )
            
            ap_50_95_scores.append(ap_50_95)
            ap_50_scores.append(ap_50)
            
            print(f"  Batch {batch_idx + 1} Results:")
            print(f"    AP@50:95: {ap_50_95:.4f}")
            print(f"    AP@50:    {ap_50:.4f}")
            print(f"    GT objects: {batch_gt_count}")
            print()
            
        except Exception as e:
            print(f"  ‚ùå Error processing batch {batch_idx + 1}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if not ap_50_95_scores:
        print("‚ùå No valid batches processed!")
        return False
    
    # Calculate overall results
    overall_ap_50_95 = np.mean(ap_50_95_scores)
    overall_ap_50 = np.mean(ap_50_scores)
    
    print("=" * 80)
    print("FINAL RESULTS - REAL DATASET WITH PERFECT PREDICTIONS")
    print("=" * 80)
    print(f"Overall AP@50:95 (mAP): {overall_ap_50_95:.4f}")
    print(f"Overall AP@50:       {overall_ap_50:.4f}")
    print(f"Total batches tested: {len(ap_50_95_scores)}")
    print(f"Total GT objects: {total_gt_objects}")
    print()
    
    # Validation checks
    success = True
    tolerance = 0.05  # Allow some tolerance for real data
    
    if overall_ap_50_95 < (1.0 - tolerance):
        print(f"‚ùå FAILED: AP@50:95 ({overall_ap_50_95:.4f}) should be close to 1.0 for perfect predictions!")
        success = False
    else:
        print(f"‚úÖ PASSED: AP@50:95 ({overall_ap_50_95:.4f}) is close to perfect score!")
    
    if overall_ap_50 < (1.0 - tolerance):
        print(f"‚ùå FAILED: AP@50 ({overall_ap_50:.4f}) should be close to 1.0 for perfect predictions!")
        success = False
    else:
        print(f"‚úÖ PASSED: AP@50 ({overall_ap_50:.4f}) is close to perfect score!")
    
    return success


def test_real_dataset_imperfect_predictions():
    """Test evaluator with imperfect predictions using real data."""
    
    print("\n" + "=" * 80)
    print("CITYSCAPES EVALUATOR TEST - REAL DATASET WITH IMPERFECT PREDICTIONS")
    print("=" * 80)
    
    # Load real dataset (smaller subset for imperfect test)
    dataset = initialize_real_cityscapes_dataset(num_samples=8)
    
    if dataset is None:
        print("‚ùå Failed to load dataset.")
        return False
    
    dataloader = DataLoader(dataset, batch_size=2, shuffle=False, collate_fn=lambda x: x)
    
    evaluator = CityscapesEvaluator(
        dataloader=dataloader,
        img_size=[512, 512],
        confthre=0.5,
        nmsthre=0.65,
        num_classes=8,
        device="cpu"
    )
    
    print("Testing with imperfect predictions on real data...")
    
    # Test one batch
    batch = next(iter(dataloader))
    
    # Extract ground truth
    batch_gt = []
    batch_img_info = []
    
    for sample_idx, sample in enumerate(batch):
        gt_bboxes = extract_ground_truth_from_sample(sample)
        batch_gt.append(gt_bboxes)
        
        img_meta = create_img_meta_from_sample(sample, sample_idx)
        batch_img_info.append(img_meta)
    
    # Create imperfect predictions with different noise levels
    noise_levels = [0, 5, 10, 20]  # pixels
    
    for noise in noise_levels:
        imperfect_preds = create_imperfect_predictions(batch_gt, noise_level=noise)
        
        try:
            ap_50_95, ap_50, summary = evaluator.calculate_coco_metrics(
                imperfect_preds, batch_gt, batch_img_info
            )
            
            print(f"Noise level {noise}px - AP@50:95: {ap_50_95:.4f}, AP@50: {ap_50:.4f}")
            
        except Exception as e:
            print(f"Error with noise level {noise}px: {e}")
    
    return True


def test_real_dataset_edge_cases():
    """Test edge cases with real dataset."""
    
    print("\n" + "=" * 80)
    print("CITYSCAPES EVALUATOR TEST - REAL DATASET EDGE CASES")
    print("=" * 80)
    
    dataset = initialize_real_cityscapes_dataset(num_samples=4)
    
    if dataset is None:
        return False
    
    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=lambda x: x)
    
    evaluator = CityscapesEvaluator(
        dataloader=dataloader,
        img_size=[512, 512],
        confthre=0.5,
        nmsthre=0.65,
        num_classes=8,
        device="cpu"
    )
    
    batch = next(iter(dataloader))
    sample = batch[0]
    
    gt_bboxes = extract_ground_truth_from_sample(sample)
    img_meta = create_img_meta_from_sample(sample, 0)
    
    edge_cases = [
        ("No predictions", [None]),
        ("Empty predictions", [torch.empty((0, 7))]),  # Fixed to 7 elements
        ("Half missing objects", [create_perfect_predictions([gt_bboxes])[0][:len(gt_bboxes)//2] if len(gt_bboxes) > 1 else None]),
    ]
    
    for case_name, predictions in edge_cases:
        print(f"Testing: {case_name}")
        try:
            ap_50_95, ap_50, summary = evaluator.calculate_coco_metrics(
                predictions, [gt_bboxes], [img_meta]
            )
            print(f"  Result: AP@50:95={ap_50_95:.4f}, AP@50={ap_50:.4f}")
        except Exception as e:
            print(f"  Error: {e}")
        print()
    
    return True


def analyze_dataset_statistics(dataset):
    """Analyze the real dataset to understand its characteristics."""
    
    print("\n" + "=" * 80)
    print("REAL DATASET ANALYSIS")
    print("=" * 80)
    
    class_counts = {}
    bbox_areas = []
    samples_with_objects = 0
    
    print("Analyzing dataset statistics...")
    
    for i in range(min(10, len(dataset))):  # Analyze first 10 samples
        try:
            sample = dataset[i] if hasattr(dataset, '__getitem__') else dataset.dataset[dataset.indices[i]]
            gt_bboxes = extract_ground_truth_from_sample(sample)
            
            if len(gt_bboxes) > 0:
                samples_with_objects += 1
                
                for bbox in gt_bboxes:
                    if len(bbox) >= 5:
                        class_id, x, y, w, h = bbox[0], bbox[1], bbox[2], bbox[3], bbox[4]
                        area = w * h
                        bbox_areas.append(float(area))
                        
                        class_id = int(class_id)
                        if class_id in class_counts:
                            class_counts[class_id] += 1
                        else:
                            class_counts[class_id] = 1
        except Exception as e:
            print(f"Error analyzing sample {i}: {e}")
            continue
    
    print(f"Dataset Statistics:")
    print(f"  Samples analyzed: {min(10, len(dataset))}")
    print(f"  Samples with objects: {samples_with_objects}")
    print(f"  Total bounding boxes: {len(bbox_areas)}")
    
    if bbox_areas:
        print(f"  Average bbox area: {np.mean(bbox_areas):.1f}")
        print(f"  Bbox area range: [{np.min(bbox_areas):.1f}, {np.max(bbox_areas):.1f}]")
    
    if class_counts:
        print(f"  Class distribution:")
        for class_id, count in sorted(class_counts.items()):
            class_name = DSEC_DET_CLASSES[class_id] if class_id < len(DSEC_DET_CLASSES) else f"class_{class_id}"
            print(f"    {class_name} ({class_id}): {count} objects")
    
    print()


def main():
    """Run all evaluator tests with real Cityscapes dataset."""
    
    print("CITYSCAPES EVALUATOR COMPREHENSIVE TEST SUITE")
    print("Using REAL Cityscapes dataset with actual images and ground truth")
    print("Using proper dataset methods to handle padding and scaling")
    print()
    
    # Check if dataset is available
    print("Checking dataset availability...")
    dataset = initialize_real_cityscapes_dataset(num_samples=5)
    
    if dataset is None:
        print("‚ùå CRITICAL: Unable to load Cityscapes dataset!")
        print("Please ensure:")
        print("  1. Cityscapes dataset is downloaded and extracted")
        print("  2. Update paths in the script or create cs_train.txt split file")
        print("  3. Dataset structure: data/cityscapes/leftImg8bit/train/ and data/cityscapes/gtFine/train/")
        return False
    
    # Analyze dataset
    analyze_dataset_statistics(dataset)
    
    # Run tests
    test_results = []
    
    try:
        # Test 1: Perfect predictions with real data
        result1 = test_real_dataset_perfect_predictions()
        test_results.append(("Perfect Predictions (Real Data)", result1))
        
        # Test 2: Imperfect predictions with real data
        result2 = test_real_dataset_imperfect_predictions()
        test_results.append(("Imperfect Predictions (Real Data)", result2))
        
        # Test 3: Edge cases with real data
        result3 = test_real_dataset_edge_cases()
        test_results.append(("Edge Cases (Real Data)", result3))
        
    except Exception as e:
        print(f"‚ùå CRITICAL ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # Summary
    print("\n" + "=" * 80)
    print("TEST SUMMARY - REAL CITYSCAPES DATASET")
    print("=" * 80)
    
    all_passed = True
    for test_name, result in test_results:
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"{test_name:35} {status}")
        if not result:
            all_passed = False
    
    print()
    if all_passed:
        print("üéâ ALL TESTS PASSED! CityscapesEvaluator works correctly with real data!")
        print("The evaluator correctly calculates mAP metrics on real Cityscapes:")
        print("  - Perfect predictions achieve mAP ‚âà 1.0")
        print("  - Imperfect predictions show realistic degradation")
        print("  - Edge cases are handled gracefully")
        print("  - Real ground truth data is processed correctly")
        print("  - Padding and scaling transformations are properly handled")
    else:
        print("‚ö†Ô∏è  SOME TESTS FAILED! Please check the evaluator implementation.")
    
    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)